# Configuración de Modelos LLM - Archivo de configuración externa
# Este archivo permite escalabilidad y mantenibilidad sin modificar código

# Configuración global
global:
  ollama_host: "http://localhost:11434"
  max_loaded_models: 2  # RTX 2070 SUPER 8GB limit
  auto_stop_inactive: true
  inactive_timeout_minutes: 30

# Modelos disponibles
models:
  qwen:
    name: "qwen2.5-coder:latest"
    description: "Code completion and programming assistance"
    category: "coding"
    size_gb: 4.7
    vram_gb: 5.0
    tokens_per_sec: 25

  deepseek:
    name: "deepseek-coder:latest"
    description: "Technical reasoning and code analysis"
    category: "reasoning"
    size_gb: 6.0
    vram_gb: 6.5
    tokens_per_sec: 20

  mistral:
    name: "mistral:latest"
    description: "General purpose and documentation"
    category: "general"
    size_gb: 4.1
    vram_gb: 4.5
    tokens_per_sec: 30

  # Modelos adicionales disponibles (descomentados según necesidad)
  # codellama:
  #   name: "codellama:latest"
  #   description: "Meta's Code Llama model"
  #   category: "coding"
  #   size_gb: 7.0
  #   vram_gb: 7.5
  #   tokens_per_sec: 18

  # llama2:
  #   name: "llama2:latest"
  #   description: "Meta's Llama 2 general purpose model"
  #   category: "general"
  #   size_gb: 3.9
  #   vram_gb: 4.0
  #   tokens_per_sec: 35