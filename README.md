# ğŸ¤– LLM Stack Manager - Local Native

**Gestor inteligente de modelos LLM para desarrollo local con RTX 2070 SUPER**

Herramienta especializada para ejecutar y gestionar modelos de lenguaje grandes (LLM) en hardware local optimizado, diseÃ±ada especÃ­ficamente para el flujo de trabajo de desarrollo de software con IA asistida.

## ğŸ¯ Objetivo del Proyecto

**Proporcionar una experiencia fluida de desarrollo con IA local** que permita:

- ğŸš€ **IteraciÃ³n rÃ¡pida**: Activar/desactivar modelos instantÃ¡neamente sin afectar otras tareas
- ğŸ® **Compatibilidad gaming**: Liberar VRAM completa cuando no se usa IA
- ğŸ’» **Desarrollo continuo**: Mantenimiento de rendimiento en codificaciÃ³n, debugging y testing
- ğŸ”§ **Control total**: GestiÃ³n granular de recursos GPU/CPU sin abstracciones complejas

### ğŸ’¡ Caso de Uso Principal

**Desarrollador que necesita IA asistida** pero tambiÃ©n juega videojuegos y ejecuta tareas intensivas de desarrollo en una RTX 2070 SUPER con 8GB VRAM.

**Problema resuelto**: La mayorÃ­a de herramientas LLM requieren contenedores Docker pesados o gestiÃ³n manual compleja que interfiere con otros usos del sistema.

## ğŸ–¥ï¸ Entorno de Desarrollo

### Hardware Objetivo
- **GPU**: NVIDIA RTX 2070 SUPER (8GB GDDR6)
- **CPU**: AMD Ryzen 5 3600XT (6 nÃºcleos, 12 hilos)
- **RAM**: 32GB DDR4 2666MHz
- **Almacenamiento**: 1TB NVMe SSD (EXT4)

### Software Base
- **OS**: Ubuntu 25.10 (Questing Quokka)
- **Kernel**: Linux 6.17+
- **NVIDIA Drivers**: 580.95+
- **CUDA**: 13.0+
- **Ollama**: Latest stable (instalado nativamente)

### ConfiguraciÃ³n Optimizada
```bash
# GPU Memory: 8GB total, ~6.5GB disponibles para modelos
# RecomendaciÃ³n: MÃ¡ximo 2 modelos simultÃ¡neos
# Modelos objetivo: 7B parameters Q4_K_M quantization
```

## âœ… Estado del Proyecto

**ğŸŸ¡ VERSIÃ“N DE PRUEBAS** - VersiÃ³n 0.0.1 con arquitectura simplificada

### ğŸ¯ CaracterÃ­sticas TÃ©cnicas Implementadas
- âœ… **Arquitectura nativa**: Ollama sin contenedores Docker
- âœ… **GestiÃ³n VRAM inteligente**: LÃ­mites automÃ¡ticos y liberaciÃ³n
- âœ… **ConfiguraciÃ³n externa**: Modelos definidos en YAML
- âœ… **Interfaz CLI moderna**: Rich library con UX fluida
- âœ… **Actualizaciones automÃ¡ticas**: DetecciÃ³n y aplicaciÃ³n de updates
- âœ… **Testing completo**: 50+ pruebas unitarias (>95% cobertura)

## ğŸš€ CaracterÃ­sticas TÃ©cnicas

### ğŸ¤– Motor de IA
- **Ollama Nativo**: EjecuciÃ³n directa sin contenedores Docker
- **API Compatible OpenAI**: Endpoint `/v1/chat/completions` para integraciÃ³n IDE
- **Modelos Optimizados**: QuantizaciÃ³n Q4_K_M para RTX 2070 SUPER
- **GestiÃ³n de Memoria**: Control granular de VRAM GPU

### ğŸ¯ GestiÃ³n Inteligente de Recursos
- **LÃ­mites VRAM**: MÃ¡ximo 2 modelos simultÃ¡neos (8GB RTX 2070)
- **ActivaciÃ³n bajo demanda**: Modelos cargados solo cuando se necesitan
- **LiberaciÃ³n automÃ¡tica**: Stop de modelos inactivos para gaming/desarrollo
- **Monitoreo real-time**: Estado de GPU, CPU y memoria

### âš™ï¸ ConfiguraciÃ³n Declarativa
- **YAML Externo**: Modelos definidos en `config/models.yml`
- **Sin base de datos**: ConfiguraciÃ³n como cÃ³digo, versionable
- **ValidaciÃ³n automÃ¡tica**: VerificaciÃ³n de configuraciÃ³n al inicio
- **Hot-reload**: Cambios aplicados sin reiniciar

### ğŸ¨ Experiencia de Usuario
- **CLI Moderna**: Rich library con colores, tablas y progreso visual
- **MenÃºs Interactivos**: NavegaciÃ³n intuitiva con indicadores visuales
- **Feedback inmediato**: Estados, progreso y errores claramente comunicados
- **Manejo de errores**: RecuperaciÃ³n automÃ¡tica y mensajes informativos

## ğŸ“‹ Requisitos del Sistema

Esta aplicaciÃ³n soporta **Linux (NVIDIA + CUDA)** y **macOS Apple Silicon (Metal)** como plataformas equivalentes con la misma arquitectura base.

### Requisitos Base (Ambas Plataformas)

- **Python**: 3.11+ con venv
- **Ollama**: Ãšltima versiÃ³n estable (instalaciÃ³n nativa)
- **Dependencias Python**: Especificadas en `requirements.txt`

La instalaciÃ³n y comportamiento en tiempo de ejecuciÃ³n estÃ¡n unificados y centralizados en la aplicaciÃ³n Python (`lib/main.py`).

### Especificaciones por Plataforma

#### Linux (NVIDIA + CUDA)
- **NVIDIA Drivers**: 580.95+ recomendado
- **CUDA**: 13.0+
- **OptimizaciÃ³n**: QuantizaciÃ³n Q4_K_M para modelos de 7B parameters
- **GestiÃ³n VRAM**: MÃ¡ximo 2 modelos simultÃ¡neos (recomendado para GPUs de 8GB)

#### macOS Apple Silicon (M1/M2/M3/M4+)
- **Metal**: AceleraciÃ³n integrada automÃ¡tica
- **Memoria unificada**: Ajusta `max_loaded_models` segÃºn memoria disponible
- **ConfiguraciÃ³n**: Perfiles especÃ­ficos en `config/app.yml` y `config/models.yml`
- **RecomendaciÃ³n**: Preferir 1 modelo grande o mÃºltiples pequeÃ±os segÃºn RAM disponible

---

### InstalaciÃ³n de Ollama y Dependencias

El launcher `./llm-stack` maneja solo el entorno virtual Python. Para instalar Ollama y dependencias:

1. **OpciÃ³n automÃ¡tica**: Ejecuta `./llm-stack` y selecciona "Instalar Dependencias" (opciÃ³n 2)
2. **OpciÃ³n manual**: Instala Ollama segÃºn tu plataforma desde https://ollama.ai

Alternativamente, export `LLM_SKIP_INSTALL=1` antes de ejecutar `./llm-stack` para omitir pasos de instalaciÃ³n interactiva.

### VerificaciÃ³n de Requisitos

```bash
# ValidaciÃ³n automÃ¡tica
./llm-stack  # Selecciona opciÃ³n 1: "ğŸ” Validar InstalaciÃ³n Completa"

# VerificaciÃ³n manual
python3 --version          # Verificar Python 3.11+
ollama --version           # Verificar Ollama instalado
```

El sistema detecta automÃ¡ticamente tu plataforma y ofrece recomendaciones especÃ­ficas de modelos y lÃ­mites de memoria.

### Dependencias Python

```txt
rich>=13.7.0        # CLI moderna
pyyaml>=6.0.0       # ConfiguraciÃ³n YAML
requests>=2.31.0    # APIs HTTP
pytest>=7.0.0       # Testing (opcional)
```

---

> **Consejo de Memoria**: 
> - **Linux/NVIDIA**: Monitorea con `nvidia-smi` durante uso intensivo
> - **macOS Apple Silicon**: La memoria es unificada; ajusta lÃ­mites en `config/app.yml` segÃºn disponibilidad

## ğŸ› ï¸ InstalaciÃ³n y ConfiguraciÃ³n

### InstalaciÃ³n (3 pasos simples)
```bash
# 1. Clonar el repositorio
git clone <repository-url>
cd local-llm-prompt

# 2. Hacer ejecutable el script launcher
chmod +x llm-stack

# 3. Ejecutar (configura todo automÃ¡ticamente)
./llm-stack
```

**Â¡Eso es todo!** El launcher script automÃ¡ticamente:
- âœ… Verifica Python 3.8+
- âœ… Crea entorno virtual (.venv)
- âœ… Instala dependencias Python
- âœ… Crea configuraciÃ³n inicial
- âœ… Inicia la aplicaciÃ³n

### VerificaciÃ³n
```bash
# Ejecutar verificaciÃ³n automÃ¡tica
./llm-stack
# Seleccionar opciÃ³n 1: "ğŸ” Validar InstalaciÃ³n Completa"
```

## ğŸ¯ GuÃ­a de Uso

### Flujo de Trabajo TÃ­pico

#### 1. **Inicio de SesiÃ³n de Desarrollo**
```bash
# Desde el directorio del proyecto
cd local-llm-prompt

# Iniciar aplicaciÃ³n (maneja venv automÃ¡ticamente)
./llm-stack
```

#### 2. **Verificar Estado del Sistema**
```
Selecciona una opciÃ³n [1/2/3/4/5/6/7/8/0]: 7

ğŸ“Š Estado del Sistema
Componente          Estado          Detalle
Servicio Ollama     âœ… Activo       http://localhost:11434
Modelos Instalados  ğŸ“¦ 3            Corriendo: 1
VRAM RTX 2070       ğŸ§  4GB / 8GB    MÃ¡x: 2 modelos
```

#### 3. **Activar Modelo para Trabajo**
```
Selecciona una opciÃ³n [1/2/3/4/5/6/7/8/0]: 3

ğŸŸ¢ ActivaciÃ³n Inteligente de Modelo
Modelos disponibles en configuraciÃ³n:
  1. qwen: qwen2.5-coder:latest - Code completion and programming
  2. deepseek: deepseek-coder:latest - Technical reasoning and analysis
  3. mistral: mistral:latest - Documentation and architecture

Selecciona modelo para activar: 1
Â¿Activar qwen2.5-coder:latest? [y/N]: y

ğŸ“¥ Modelo qwen2.5-coder:latest no instalado, descargando...
ğŸ§ª Activando modelo: qwen2.5-coder:latest
âœ… Modelo qwen2.5-coder:latest activado exitosamente
ğŸ’¡ El modelo estÃ¡ listo para usar en VSCode/Kilo Code
```

#### 4. **Trabajar con IA en el IDE**
- Abrir VSCode/Kilo Code
- Configurar provider: Custom OpenAI
- Base URL: `http://localhost:11434/v1`
- Model: `qwen2.5-coder:latest`
- API Key: `ollama`

#### 5. **Liberar Recursos para Gaming/Testing**
```
Selecciona una opciÃ³n [1/2/3/4/5/6/7/8/0]: 4

ğŸ›‘ Desactivando Modelo
Modelos activos:
  1. qwen2.5-coder:latest

Selecciona modelo para desactivar: 1
Â¿Desactivar qwen2.5-coder:latest y liberar VRAM? [y/N]: y

âœ… Modelo qwen2.5-coder:latest desactivado
ğŸ’¾ VRAM liberada para gaming o otros modelos
```

### Modelos Optimizados para RTX 2070 SUPER

| Modelo | Comando | VRAM | Uso Principal | Estado |
|--------|---------|------|---------------|--------|
| **Qwen2.5-Coder-7B** | `qwen` | ~5GB | Code completion | âœ… Recomendado |
| **DeepSeek-Coder-V2-Lite** | `deepseek` | ~6.5GB | Technical reasoning | âœ… Recomendado |
| **Mistral-7B-Instruct** | `mistral` | ~4.5GB | Documentation | âš ï¸ Solo si VRAM libre |

### âš ï¸ LÃ­mites de Hardware
- **MÃ¡ximo 2 modelos simultÃ¡neos** en RTX 2070 SUPER 8GB
- **Liberar VRAM antes de gaming** para rendimiento Ã³ptimo
- **Monitorear temperatura GPU** durante uso intensivo

## ğŸ—ï¸ Arquitectura TÃ©cnica

### DiseÃ±o Simplificado
```
Usuario â†’ CLI Rich â†’ ConfigManager â†’ OllamaManager â†’ Ollama CLI â†’ GPU
```

### Componentes Core

#### ğŸ¤– OllamaManager
**Responsabilidad**: Interfaz directa con Ollama CLI
- âœ… GestiÃ³n de modelos (pull, run, stop, rm)
- âœ… Control de VRAM y lÃ­mites automÃ¡ticos
- âœ… DetecciÃ³n de actualizaciones desde registry
- âœ… Monitoreo de estado GPU/CPU

#### âš™ï¸ ConfigManager
**Responsabilidad**: ConfiguraciÃ³n externa YAML
- âœ… Carga de `config/models.yml` y `config/app.yml`
- âœ… ValidaciÃ³n de configuraciÃ³n y modelos
- âœ… CreaciÃ³n automÃ¡tica de archivos por defecto
- âœ… Sin base de datos, configuraciÃ³n como cÃ³digo

#### ğŸ¨ CLI Interface
**Responsabilidad**: UX moderna y navegaciÃ³n
- âœ… MenÃºs interactivos con Rich library
- âœ… Estados visuales y progreso de operaciones
- âœ… Manejo de errores y recuperaciÃ³n automÃ¡tica
- âœ… Feedback inmediato para todas las operaciones

### ConfiguraciÃ³n Declarativa

```yaml
# config/models.yml
global:
  ollama_host: "http://localhost:11434"
  max_loaded_models: 2
  auto_stop_inactive: true

models:
  qwen:
    name: "qwen2.5-coder:latest"
    description: "Code completion and programming"
  deepseek:
    name: "deepseek-coder:latest"
    description: "Technical reasoning and analysis"
```

## ğŸ§ª Calidad y Testing

### Suite de Pruebas Completa
```bash
# Ejecutar todas las pruebas
pytest lib/__tests__/ -v

# Con reporte de cobertura
pytest lib/__tests__/ --cov=lib --cov-report=html
```

**ğŸ“Š MÃ©tricas**: 50+ pruebas unitarias, >95% cobertura, mocks completos

### Componentes Testeados
- âœ… **ConfigManager**: Carga YAML, validaciÃ³n, configuraciÃ³n por defecto
- âœ… **OllamaManager**: GestiÃ³n modelos, VRAM, actualizaciones automÃ¡ticas
- âœ… **CLI Interface**: UX, navegaciÃ³n, manejo de errores
- âœ… **IntegraciÃ³n**: Workflows completos end-to-end

## ğŸ“ Estructura del Proyecto

```
local-llm-prompt/
â”œâ”€â”€ config/                 # âš™ï¸ ConfiguraciÃ³n externa
â”‚   â”œâ”€â”€ models.yml         # Modelos disponibles
â”‚   â””â”€â”€ app.yml           # ConfiguraciÃ³n aplicaciÃ³n
â”œâ”€â”€ lib/                   # ğŸ“¦ CÃ³digo fuente
â”‚   â”œâ”€â”€ main.py           # ğŸš€ CLI principal
â”‚   â”œâ”€â”€ config_manager.py # âš™ï¸ GestiÃ³n YAML
â”‚   â”œâ”€â”€ ollama_manager.py # ğŸ¤– Cliente Ollama
â”‚   â””â”€â”€ __tests__/        # ğŸ§ª Tests unitarios
â”œâ”€â”€ specs/                # ğŸ“‹ DocumentaciÃ³n tÃ©cnica
â”œâ”€â”€ requirements.txt      # ğŸ“¦ Dependencias
â””â”€â”€ README.md            # ğŸ“– Esta guÃ­a
```

## âš™ï¸ ConfiguraciÃ³n Kilo Code / VSCode

DespuÃ©s de activar un modelo, configÃºralo en tu IDE:

**ConfiguraciÃ³n OpenAI Compatible:**
- **Provider**: Custom OpenAI
- **Base URL**: `http://localhost:11434/v1`
- **API Key**: `ollama`
- **Model**: Nombre del modelo activado (ej: `qwen2.5-coder:latest`)

## ğŸš¨ Troubleshooting

### Problemas Comunes

**Ollama no responde:**
```bash
# Verificar servicio
ollama list

# Reiniciar si es necesario
ollama serve
```

**Modelo no carga:**
```bash
# Verificar VRAM disponible
nvidia-smi

# Detener otros modelos
./llm-stack  # OpciÃ³n 4 (Desactivar Modelo)
```

**ConfiguraciÃ³n corrupta:**
```bash
# Resetear configuraciÃ³n
rm -rf config/
./llm-stack  # RecrearÃ¡ archivos automÃ¡ticamente
```

## ğŸ¯ PrÃ³ximos Pasos

### **Inicio RÃ¡pido**
1. **Clonar**: `git clone <repo> && cd local-llm-prompt`
2. **Ejecutar**: `chmod +x llm-stack && ./llm-stack`
3. **Â¡Listo!** Todo se configura automÃ¡ticamente

### **ğŸ”® ExpansiÃ³n Futura: macOS Apple Silicon**
PrÃ³xima versiÃ³n incluirÃ¡ soporte completo para MacBook Pro/Max con chips M1/M2/M3/M4, expandiendo el alcance a ~30% del mercado de desarrollo.

## ğŸ’¡ Consejos para RTX 2070 SUPER

- **MÃ¡ximo 2 modelos simultÃ¡neos** (8GB VRAM lÃ­mite)
- **Liberar VRAM para gaming** deteniendo modelos activos
- **Monitorear temperatura** durante uso intensivo
- **Actualizaciones automÃ¡ticas** disponibles en el menÃº

---

**ğŸš€ Â¡Listo para desarrollo fluido con IA local!**

**ğŸ”® PrÃ³xima expansiÃ³n: Soporte macOS Apple Silicon** ğŸ