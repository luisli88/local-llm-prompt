# LLM Local Stack Manager

Aplicaci√≥n para gestionar modelos de lenguaje locales usando Docker y Ollama con interfaz Python moderna.

## ‚úÖ Estado del Proyecto

**üü¢ COMPLETAMENTE FUNCIONAL** - Aplicaci√≥n Python lista para uso en producci√≥n.

### Problemas Resueltos (v2.0.0)
- ‚úÖ Validaci√≥n de instalaci√≥n funciona correctamente
- ‚úÖ Instalaci√≥n del stack inicia contenedores con Docker Compose
- ‚úÖ Activaci√≥n/desactivaci√≥n de modelos funciona perfectamente
- ‚úÖ Actualizaci√≥n de modelos con l√≥gica corregida
- ‚úÖ Interfaz moderna con Rich y navegaci√≥n fluida
- ‚úÖ Sincronizaci√≥n autom√°tica DB ‚Üî Docker
- ‚úÖ Suite completa de pruebas unitarias (13/13)

### Mejoras vs Versi√≥n Bash
- **Precisi√≥n**: APIs nativas de Docker vs parsing de CLI
- **Robustez**: Manejo avanzado de errores y recuperaci√≥n autom√°tica
- **Mantenibilidad**: Arquitectura modular y testable
- **UX**: Interfaz moderna con colores, tablas y progreso
- **Confiabilidad**: Validaci√≥n autom√°tica de dependencias y estado

## üöÄ Caracter√≠sticas

- **Interfaz Moderna**: CLI interactiva con Rich para una experiencia de usuario mejorada
- **Gesti√≥n Inteligente**: Estado autom√°tico de contenedores y modelos
- **Base de Datos SQLite**: Persistencia de configuraciones y versiones
- **Sincronizaci√≥n en Tiempo Real**: Estado actualizado autom√°ticamente
- **Validaci√≥n Robusta**: Verificaci√≥n completa de dependencias y conectividad
- **Actualizaciones Autom√°ticas**: Pull de nuevas versiones de modelos

## üìã Requisitos

- **Ubuntu 25.10** (o compatible)
- **Docker** con NVIDIA Container Toolkit
- **Python 3.11+** con entorno virtual
- **RTX 2070 SUPER** o GPU NVIDIA compatible (8GB+ VRAM)

## üõ†Ô∏è Instalaci√≥n

### 1. Clonar el repositorio
```bash
git clone <repository-url>
cd llm-local-stack
```

### 2. Crear entorno virtual
```bash
python3 -m venv venv
source venv/bin/activate
```

### 3. Instalar dependencias
```bash
pip install -r requirements.txt
```

### 4. Instalar stack base
```bash
# Ejecutar instalaci√≥n inicial
python main.py
# Seleccionar opci√≥n 2: "Instalar Stack"
```

## üéØ Uso

### Interfaz Interactiva
```bash
python main.py
```

### Men√∫ Principal
1. **Validar Instalaci√≥n**: Verifica estado completo del stack
2. **Instalar Stack**: Configuraci√≥n inicial de contenedores
3. **Activar Modelo**: Inicia contenedor de modelo espec√≠fico
4. **Desactivar Modelo**: Detiene contenedor de modelo
5. **Actualizar Modelos**: Descarga √∫ltimas versiones
6. **Desactivar Stack Completo**: Detiene todos los contenedores

### Modelos Disponibles

| Modelo | Contenedor | Puerto | Uso |
|--------|------------|--------|-----|
| Qwen2.5-Coder | ollama-qwen | 11434 | Code completion |
| DeepSeek-Coder | ollama-deepseek | 11435 | Technical reasoning |
| Mistral | ollama-mistral | 11436 | Documentation |

## üèóÔ∏è Arquitectura

### Componentes Principales

#### DockerManager (`docker_manager.py`)
- Comunicaci√≥n directa con Docker API
- Gesti√≥n del ciclo de vida de contenedores
- Estad√≠sticas de uso en tiempo real
- Validaci√≥n de salud de servicios

#### ModelManager (`model_manager.py`)
- Base de datos SQLite con SQLAlchemy
- Sincronizaci√≥n autom√°tica DB ‚Üî Docker
- Tracking de versiones instaladas
- Operaciones CRUD de modelos

#### OllamaClient (`ollama_client.py`)
- Cliente para operaciones con Ollama
- Gesti√≥n de modelos: pull, list, test
- Validaci√≥n de funcionamiento
- Actualizaciones autom√°ticas

#### CLI Interface (`main.py`)
- Interfaz de usuario con Rich
- Men√∫s interactivos y coloreados
- Gesti√≥n de flujo de navegaci√≥n
- Presentaci√≥n de informaci√≥n en tiempo real

### Base de Datos

```sql
-- Esquema de la base de datos
CREATE TABLE models (
    name TEXT PRIMARY KEY,
    container_name TEXT UNIQUE,
    port INTEGER,
    installed_version TEXT,
    status TEXT DEFAULT 'inactive',
    last_updated DATETIME,
    created_at DATETIME
);
```

## üß™ Testing

### Ejecutar Pruebas
```bash
# Todas las pruebas
pytest tests.py -v

# Pruebas espec√≠ficas
pytest tests.py::TestDockerManager -v

# Con coverage
pytest --cov=. --cov-report=html
```

### Tipos de Pruebas
- **Unitarias**: Funciones individuales
- **Integraci√≥n**: Flujo completo con Docker
- **Mocks**: Simulaci√≥n de APIs externas

## üìÅ Estructura del Proyecto

```
llm-stack-manager/
‚îú‚îÄ‚îÄ main.py                 # CLI principal
‚îú‚îÄ‚îÄ docker_manager.py       # Gesti√≥n Docker
‚îú‚îÄ‚îÄ model_manager.py        # Gesti√≥n modelos/DB
‚îú‚îÄ‚îÄ ollama_client.py        # Cliente Ollama
‚îú‚îÄ‚îÄ models.py              # Modelos SQLAlchemy
‚îú‚îÄ‚îÄ config.py              # Configuraci√≥n
‚îú‚îÄ‚îÄ utils.py               # Utilidades
‚îú‚îÄ‚îÄ tests.py               # Pruebas unitarias
‚îú‚îÄ‚îÄ requirements.txt       # Dependencias Python
‚îú‚îÄ‚îÄ .models.db            # Base de datos SQLite
‚îú‚îÄ‚îÄ docker-compose.yml    # Configuraci√≥n Docker
‚îî‚îÄ‚îÄ .scripts/             # Scripts auxiliares
    ‚îú‚îÄ‚îÄ setup.sh
    ‚îú‚îÄ‚îÄ verify-install.sh
    ‚îî‚îÄ‚îÄ model_manager.sh
```

## üîß Configuraci√≥n Avanzada

### Variables de Entorno
```bash
# Personalizar rutas
export LLM_DB_PATH="/custom/path/.models.db"
export LLM_SCRIPTS_DIR="/custom/scripts"

# Configuraci√≥n Docker
export DOCKER_HOST="unix:///var/run/docker.sock"
```

### Configuraci√≥n de Modelos
Editar `config.py` para agregar nuevos modelos:

```python
MODELS_CONFIG.update({
    "nuevo-modelo": {
        "name": "Nuevo Modelo",
        "container_name": "ollama-nuevo",
        "port": 11437,
        "description": "Descripci√≥n del modelo"
    }
})
```

## üö® Soluci√≥n de Problemas

### Problemas Comunes

#### Docker no disponible
```bash
# Verificar servicio Docker
sudo systemctl status docker
sudo systemctl start docker

# Verificar permisos de usuario
sudo usermod -aG docker $USER
# Reiniciar sesi√≥n
```

#### GPU no detectada
```bash
# Verificar NVIDIA drivers
nvidia-smi

# Verificar toolkit
docker run --rm --gpus all nvidia/cuda:11.0-base nvidia-smi
```

#### Puertos ocupados
```bash
# Verificar puertos en uso
netstat -tlnp | grep :11434

# Cambiar puertos en config.py
```

### Logs y Debugging
```bash
# Ver logs de contenedores
docker compose logs ollama-qwen

# Ver estado detallado
docker compose ps -a

# Debug de la aplicaci√≥n
python -c "from docker_manager import DockerManager; dm = DockerManager(); print(dm.get_stack_status())"
```

## üìä M√©tricas y Monitoreo

### Informaci√≥n de Rendimiento
- **Uso de GPU**: Memoria, temperatura, utilization
- **Uso de CPU/RAM**: Por contenedor
- **Latencia**: Tiempo de respuesta de modelos
- **Tokens/segundo**: Rendimiento de inferencia

### Comandos de Monitoreo
```bash
# Estado del stack
python -c "from main import LLMStackApp; app = LLMStackApp(); print(app._show_menu())"

# Estad√≠sticas de contenedores
docker stats

# Uso de GPU
nvidia-smi --query-gpu=utilization.gpu,utilization.memory --format=csv
```

## ü§ù Contribuci√≥n

1. Fork el proyecto
2. Crear rama para feature (`git checkout -b feature/nueva-funcionalidad`)
3. Commit cambios (`git commit -am 'Agrega nueva funcionalidad'`)
4. Push a la rama (`git push origin feature/nueva-funcionalidad`)
5. Crear Pull Request

### Gu√≠as de Desarrollo
- **Tests**: Todas las funciones deben tener tests unitarios
- **Documentaci√≥n**: Actualizar README y docstrings
- **Commits**: Mensajes descriptivos en ingl√©s
- **Style**: Seguir PEP 8

## üìÑ Licencia

Este proyecto est√° bajo la Licencia MIT. Ver archivo `LICENSE` para m√°s detalles.

## üôè Agradecimientos

- [Ollama](https://ollama.ai/) - Motor de inferencia
- [Docker](https://docker.com/) - Contenedorizaci√≥n
- [NVIDIA](https://nvidia.com/) - Aceleraci√≥n GPU
- [Rich](https://rich.readthedocs.io/) - CLI moderna
- [SQLAlchemy](https://sqlalchemy.org/) - ORM de base de datos

**Code Completion:**
- Base URL: `http://localhost:11434/v1`
- Model: `qwen2.5-coder:7b-instruct-q4_K_M`

**Technical Reasoning:**
- Base URL: `http://localhost:11435/v1`
- Model: `deepseek-coder-v2-lite-instruct-q4_K_M`

**Documentation:**
- Base URL: `http://localhost:11436/v1`
- Model: `mistral:7b-instruct-v0.3-q4_K_M`

API Key: `ollama` (para todos)

## Archivos

- `main.sh`: **Aplicaci√≥n principal** - Punto de entrada √∫nico
- `.scripts/`: Scripts de automatizaci√≥n (ocultos)
- `specs/`: Especificaciones t√©cnicas
- `docker-compose.yml`: Configuraci√≥n multi-contenedor
- `README.md`: Esta documentaci√≥n