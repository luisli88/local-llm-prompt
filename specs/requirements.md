# Especificaci√≥n: Stack LLM Local para Desarrollo (RTX 2070 + Ubuntu 25.10)

## 1. Requisitos funcionales

### RF-01: Inferencia GPU acelerada
- **Descripci√≥n**: Ejecutar modelos 7B-8B con rendimiento interactivo (>15 tokens/s)
- **Hardware**: RTX 2070 SUPER 8GB m√≠nimo
- **Formatos**: GGUF Q4_K_M / Q5_K_M
- **Modelos objetivo**: Qwen2.5-Coder-7B, DeepSeek-Coder-6.7B, Mistral-7B-Instruct

### RF-02: API compatible OpenAI
- **Descripci√≥n**: Endpoint `/v1/chat/completions` para integraci√≥n VSCode/Kilo Code
- **Protocolo**: HTTP JSON (OpenAI spec)
- **Autenticaci√≥n**: API key opcional (dummy si se requiere)
- **Puertos**: 11434 (Ollama default)

### RF-04: Gesti√≥n de Estado Persistente
- **Descripci√≥n**: Tracking autom√°tico del estado de modelos y versiones instaladas
- **Base de datos**: SQLite local (.models.db)
- **Sincronizaci√≥n**: Estado real de contenedores vs base de datos
- **Funciones**: Activar/desactivar modelos, actualizar versiones, mostrar estado

### RF-05: Interfaz de Usuario Mejorada
- **Descripci√≥n**: Men√∫ interactivo con informaci√≥n en tiempo real
- **Caracter√≠sticas**: 
  - Indicadores visuales de estado (‚óè activo, ‚óã inactivo)
  - URLs de modelos activos
  - Versiones instaladas
  - Validaci√≥n autom√°tica de dependencias

### RF-06: Actualizaci√≥n de Modelos
- **Descripci√≥n**: Pull autom√°tico de nuevas versiones desde Ollama registry
- **Alcance**: Modelo individual o actualizaci√≥n masiva
- **Estado**: Detiene contenedores temporalmente durante actualizaci√≥n

### RF-08: Detecci√≥n Autom√°tica de Actualizaciones
- **Descripci√≥n**: Verificaci√≥n autom√°tica de versiones m√°s recientes disponibles
- **Mec√°nica**: Consulta API de Ollama registry vs versiones locales
- **Notificaciones**: Indicadores visuales en interfaz de usuario
- **Acciones**: Actualizaci√≥n manual o autom√°tica de modelos

## 2. Requisitos no funcionales

### RNF-01: Performance
| M√©trica | Valor m√≠nimo | Modelo referencia |
|---------|-------------|------------------|
| Tokens/s | 20+ | 7B Q4 (Qwen2.5-Coder) |
| Time-to-first-token | <2s | Contexto 2k tokens |
| Latencia respuesta | <500ms | Single request |

### RNF-02: Disponibilidad
- **Uptime**: 100% cuando contenedor activo
- **Recuperaci√≥n**: `docker compose up -d` (<30s desde apagado)

### RNF-03: Portabilidad
- **OS**: Ubuntu 24.04/25.10 (CUDA 12.x)
- **Docker**: Multi-arch (x86_64)
- **Vol√∫menes**: Persistentes (~/.ollama/models)

## 3. Arquitectura t√©cnica

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ VSCode/Kilo ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Terminal App ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Ollama APIs ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ llama.cpp ‚îÇ
‚îÇ (OpenAI API) ‚îÇ ‚îÇ (main.sh) ‚îÇ ‚îÇ (Multi-container) ‚îÇ ‚îÇ (CUDA) ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îÇ ‚îÇ ‚îÇ ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Scripts Ocultos ‚îÇ ‚îÇ Contenedor Qwen ‚îÇ ‚îÇ Contenedor DeepSeek ‚îÇ ‚îÇ Contenedor Mistral ‚îÇ
‚îÇ (.scripts/) ‚îÇ ‚îÇ 7B Code ‚îÇ ‚îÇ 6.7B Technical ‚îÇ ‚îÇ 7B Docs ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
                       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                       ‚îÇ RTX 2070 SUPER ‚îÇ
                       ‚îÇ 8GB VRAM ‚îÇ
                       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Componentes principales:
- **Terminal App (main.sh/main.py)**: Interfaz principal con men√∫ interactivo y sincronizaci√≥n autom√°tica
- **Scripts Ocultos (.scripts/)**: Automatizaci√≥n de instalaci√≥n, gesti√≥n y model_manager.sh
- **Base de Datos SQLite (.models.db)**: Persistencia de estado y versiones de modelos
- **Contenedores Ollama**: Modelos especializados por funci√≥n con estado sincronizado
- **Validaci√≥n Autom√°tica**: Chequeo de dependencias y estado de contenedores al iniciar

## 4. Configuraci√≥n Docker Compose

## 4. Modos de Despliegue Soportados

### ü•á **MODO PRINCIPAL: Single Container** (Recomendado)
```yaml
services:
  ollama-single:
    image: ollama/ollama:latest
    container_name: ollama-single
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    ports:
      - "11434:11434"
    volumes:
      - ollama_models:/root/.ollama
    environment:
      - OLLAMA_NUM_GPU_LAYERS=35
      - OLLAMA_MAX_LOADED_MODELS=3  # Qwen+DeepSeek+Mistral
      - OLLAMA_KEEP_ALIVE=5m       # 5min idle ‚Üí unload
    shm_size: 8gb
```

**Gesti√≥n**: `docker exec ollama-single ollama pull/rm/list`

### ü•à **MODO LOCAL: Ollama Nativo** (Sin Docker)
```bash
curl -fsSL https://ollama.com/install.sh | sh
systemctl --user enable --now ollama
```
**Gesti√≥n**: `ollama pull/rm/list` directo (menor overhead)
**Puerto**: 11434 (host local)

### ü•â **MODO LEGACY: Multi-Container** (Actual)
**Uso**: Solo si necesitas aislamiento total por modelo
**Limitaci√≥n**: 3 contenedores = 3x overhead + complejidad

### Criterios de Selecci√≥n
Escenario | Single Container | Local Ollama | Multi-Container |
|-----------|------------------|--------------|-----------------|
**Simplicidad** | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê |
**Aislamiento** | ‚≠ê‚≠ê | ‚≠ê | ‚≠ê‚≠ê‚≠ê |
**Overhead** | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê |
**Migraci√≥n f√°cil** | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê |

  ollama-deepseek:
    image: ollama/ollama:latest
    container_name: ollama-deepseek
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    ports:
      - "11435:11434"
    volumes:
      - ollama_models:/root/.ollama
    environment:
      - OLLAMA_NUM_GPU_LAYERS=35
      - OLLAMA_MAX_LOADED_MODELS=1
    shm_size: 8gb

  ollama-mistral:
    image: ollama/ollama:latest
    container_name: ollama-mistral
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    ports:
      - "11436:11434"
    volumes:
      - ollama_models:/root/.ollama
    environment:
      - OLLAMA_NUM_GPU_LAYERS=35
      - OLLAMA_MAX_LOADED_MODELS=1
    shm_size: 8gb

volumes:
  ollama_models:
```

## RF-07: Gesti√≥n Inteligente de VRAM
- **Descripci√≥n**: Control autom√°tico de modelos cargados para RTX 2070 SUPER 8GB
- **Mec√°nica**:
  - M√°ximo 2 modelos simult√°neos (configurable)
  - `ollama stop <model>` autom√°tico para liberar VRAM
  - Monitoreo continuo de uso GPU
  - Auto-stop de modelos inactivos

## 5. Interfaz de Usuario y Gesti√≥n

### IU-01: App de Terminal Interactiva
- **Descripci√≥n**: Interfaz de l√≠nea de comandos con men√∫ para todas las operaciones
- **Archivo principal**: `main.sh`
- **Validaci√≥n autom√°tica**: Chequeo de dependencias al iniciar
- **Gesti√≥n de actualizaciones**: Detecci√≥n y oferta de actualizaci√≥n de componentes desactualizados

### IU-02: Men√∫ de Operaciones
- **Validar Instalaci√≥n**: Verificaci√≥n completa de dependencias y estado
- **Instalar Stack**: Setup autom√°tico de Docker, NVIDIA CTK y configuraci√≥n
- **Activar Modelo**: Selecci√≥n y activaci√≥n de modelo espec√≠fico
- **Desactivar Modelo**: Selecci√≥n y parada de modelo espec√≠fico
- **Desactivar Stack Completo**: Apagado total y liberaci√≥n de recursos

### IU-03: Scripts Ocultos
- **Ubicaci√≥n**: Carpeta `.scripts/` (oculta al usuario)
- **Acceso**: Solo a trav√©s de la app principal
- **Funciones**: Automatizaci√≥n de instalaci√≥n, validaci√≥n y gesti√≥n

## 5. Modelos recomendados (priorizados)

| Prioridad | Modelo | Tama√±o disco | VRAM Q4 | Tokens/s estimado | Uso principal |
|-----------|--------|-------------|---------|-------------------|---------------|
| 1 | Qwen2.5-Coder-7B-Instruct-Q4_K_M | ~5GB | 6.5GB | 25-35 | Code completion |
| 2 | DeepSeek-Coder-V2-Lite-Instruct-Q4_K_M | ~6GB | 7GB | 20-30 | Technical reasoning |
| 3 | Mistral-7B-Instruct-v0.3-Q4_K_M | ~4.5GB | 6GB | 30-40 | Docs/architecture |

## 6. Comandos de operaci√≥n

### Interfaz Principal

```bash
# Ejecutar la app de terminal
./main.sh
```

La app proporciona un men√∫ interactivo con las siguientes opciones:

1. **Validar Instalaci√≥n**: Verifica estado de dependencias y ofrece actualizaciones
2. **Instalar Stack**: Setup completo automatizado
3. **Activar Modelo**: Selecciona modelo a activar (Qwen/DeepSeek/Mistral)
4. **Desactivar Modelo**: Selecciona modelo a detener
5. **Desactivar Stack Completo**: Apaga todo y libera recursos

### Configuraci√≥n Kilo Code

**Para Code Completion:**
- Provider: Custom OpenAI
- Base URL: http://localhost:11434/v1
- Model: qwen2.5-coder:7b-instruct-q4_K_M
- API Key: ollama

**Para Technical Reasoning:**
- Provider: Custom OpenAI
- Base URL: http://localhost:11435/v1
- Model: qwen2.5-coder:7b-instruct-q4_K_M
- API Key: ollama

**Para Technical Reasoning:**
- Provider: Custom OpenAI
- Base URL: http://localhost:11435/v1
- Model: deepseek-coder-v2-lite-instruct-q4_K_M
- API Key: ollama

**Para Docs/Architecture:**
- Provider: Custom OpenAI
- Base URL: http://localhost:11436/v1
- Model: mistral:7b-instruct-v0.3-q4_K_M
- API Key: ollama

### Apagado gaming

```bash
# Apagar todos
docker compose down

# O apagar individualmente
docker compose down ollama-qwen
docker compose down ollama-deepseek
docker compose down ollama-mistral

# Verificar VRAM libre
nvidia-smi
```

## 7. M√©tricas de √©xito

- ‚úÖ `docker run --gpus all nvidia/cuda:12.4.0-base nvidia-smi` OK
- ‚úÖ `curl http://localhost:11434/api/generate` (Qwen) responde
- ‚úÖ `curl http://localhost:11435/api/generate` (DeepSeek) responde
- ‚úÖ `curl http://localhost:11436/api/generate` (Mistral) responde
- ‚úÖ Kilo Code autocompleta c√≥digo en <1s (cambiar endpoint seg√∫n tarea)
- ‚úÖ `docker compose down` libera 100% VRAM en <10s

